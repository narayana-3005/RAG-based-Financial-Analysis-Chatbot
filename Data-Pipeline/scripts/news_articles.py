# -*- coding: utf-8 -*-
"""worked1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yZ_3iSbqlZk8h46WS3tbh9VLTYwgB1sN
"""

import pkg_resources
import importlib
importlib.reload(pkg_resources)

import tensorflow as tf
import tensorflow_data_validation as tfdv
print('TF version:', tf.__version__)
print('TFDV version:', tfdv.version.__version__)

"""# here on wards to connect with cloud storage and validation part:

---


"""

from google.colab import drive
drive.mount('/content/drive')

json_file_path = '/content/drive/My Drive/Colab Notebooks/theta-function-429605-j0-7e0753216ae2.json'

import os
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = json_file_path

from google.cloud import storage
storage_client = storage.Client()

from google.cloud import storage
import json
import tensorflow_data_validation as tfdv
import os

# Initialize the client
client = storage.Client()

# Bucket name and paths
bucket_name = 'stock_prices-bucket'
bucket = client.get_bucket(bucket_name)

def load_json_files_from_gcs(bucket, folder_path):
    blobs = bucket.list_blobs(prefix=folder_path)
    data = []
    for blob in blobs:
        if blob.name.endswith('.json'):
            content = blob.download_as_string()
            data.append(json.loads(content))
    return data

# Example paths to historical and real-time data folders
historical_data = load_json_files_from_gcs(bucket, 'historical')
real_time_data = load_json_files_from_gcs(bucket, 'real-time')

for i, record in enumerate(historical_data[:5]):  # Print the first 5 records
    print(f"Record {i}: {record}")

"""# *News_Articles Data validation *

1. basic data quality
2. schema conformity
3. domain validation



*   Statistics Generation: Generates statistical data about each feature in your DataFrame.
*   Schema Inference: Based on these statistics, TFDV infers the schema automatically, including data types, required columns, and value ranges.
*      Validation: Compares the data against the schema to detect anomalies or mismatches.
"""

from google.cloud import storage
import json
import pandas as pd

# Initialize the GCS client
client = storage.Client()
bucket_name = 'news_articles-bucket'  # Update with your bucket name
bucket = client.get_bucket(bucket_name)

# List blobs (files) in the bucket
blobs = bucket.list_blobs()
data_list = []

# Set a limit for the number of files to process
file_limit = 100  # Adjust the limit as needed
processed_files = 0

# Iterate over each blob in the bucket
for blob in blobs:
    if processed_files >= file_limit:
        print(f"Reached the file processing limit of {file_limit}.")
        break
    # Only process JSON files
    if blob.name.endswith(".json"):
        print(f"Processing file: {blob.name}")
        json_data = blob.download_as_text()
        try:
            # Parse JSON and append data to the list
            data = json.loads(json_data)
            if isinstance(data, list):
                data_list.extend(data)  # Add all records in the list
            elif isinstance(data, dict):
                data_list.append(data)  # Add single record
            else:
                print(f"Unexpected JSON format in {blob.name}, skipping.")
            processed_files += 1
        except json.JSONDecodeError:
            print(f"Error decoding JSON in {blob.name}, skipping.")

# Convert list of dictionaries to a DataFrame if there is data
if data_list:
    df = pd.DataFrame(data_list)
    print("Data loaded into DataFrame successfully.")
    print(df.head())  # Display the first few rows of the DataFrame
else:
    print("No valid JSON data found in the bucket.")



import tensorflow_data_validation as tfdv

# Function to perform schema validation
def schema_conformity_check(data):
    # Generate statistics from the data
    stats = tfdv.generate_statistics_from_dataframe(data)

    # Infer a schema from the statistics
    schema = tfdv.infer_schema(stats)

    # Validate statistics against the inferred schema to check for anomalies
    anomalies = tfdv.validate_statistics(stats, schema)

    if anomalies.anomaly_info:
        print("Schema anomalies detected:")
        for feature_name, anomaly in anomalies.anomaly_info.items():
            print(f"Feature '{feature_name}' - {anomaly.description}")
    else:
        print("Data conforms to the schema.")

# Basic data quality checks
def basic_data_quality_checks(data):
    # Check for missing values
    missing_values = data.isnull().sum()
    print("Missing values per column:\n", missing_values)

    # Check for any missing values
    if missing_values.any():
        print("Warning: Data has missing values.")
    else:
        print("No missing values detected.")

# Domain validation checks (example: no negative values for 'Volume')
def domain_validation(data):
    # Check for negative 'Volume' values
    if 'Volume' in data.columns:
        if (data["Volume"] < 0).any():
            print("Anomaly detected: Negative values in 'Volume'.")
        else:
            print("'Volume' values are within expected range.")
    else:
        print("'Volume' column not found in the data.")

# Run data validation checks
if 'df' in locals():  # Ensure data is loaded into DataFrame 'df'
    print("\n--- Schema Conformity Check ---")
    schema_conformity_check(df)

    print("\n--- Basic Data Quality Checks ---")
    basic_data_quality_checks(df)

    print("\n--- Domain Validation ---")
    domain_validation(df)
else:
    print("Data not loaded for validation.")

import tensorflow_data_validation as tfdv

# Function to generate statistics and infer schema with TFDV
def generate_schema_with_tfdv(data):
    # Step 1: Generate statistics from the DataFrame
    stats = tfdv.generate_statistics_from_dataframe(data)
    print("Data statistics generated successfully.")

    # Step 2: Infer a schema from the statistics
    schema = tfdv.infer_schema(stats)
    print("Schema inferred successfully.")

    # Optionally, display the schema (useful for understanding data structure)
    tfdv.display_schema(schema)

    return stats, schema

# Function to validate data using inferred schema
def validate_data_with_schema(data, schema):
    # Generate statistics from the current data
    stats = tfdv.generate_statistics_from_dataframe(data)

    # Validate statistics against the provided schema
    anomalies = tfdv.validate_statistics(stats, schema)

    if anomalies.anomaly_info:
        print("Data anomalies detected:")
        for feature_name, anomaly in anomalies.anomaly_info.items():
            print(f"Feature '{feature_name}' - {anomaly.description}")
    else:
        print("Data conforms to the schema.")

# Run schema generation and validation
try:
    stats, schema = generate_schema_with_tfdv(df)  # 'df' is the DataFrame containing your data
    validate_data_with_schema(df, schema)
except Exception as e:
    print("Error during schema generation or validation:", e)

import tensorflow_data_validation as tfdv

# Function to generate statistics and infer schema with TFDV
def generate_schema_with_tfdv(data):
    try:
        # Step 1: Generate statistics from the DataFrame
        stats = tfdv.generate_statistics_from_dataframe(data)
        print("Data statistics generated successfully.")

        # Step 2: Infer a schema from the statistics
        schema = tfdv.infer_schema(stats)
        print("Schema inferred successfully.")

        # Display schema (for debugging and understanding structure)
        tfdv.display_schema(schema)

        return stats, schema
    except Exception as e:
        print("Error in generating statistics or schema:", e)
        return None, None

# Function to validate data using inferred schema
def validate_data_with_schema(data, schema):
    if schema is None:
        print("No valid schema available for validation.")
        return

    try:
        # Generate statistics from the current data
        stats = tfdv.generate_statistics_from_dataframe(data)

        # Validate statistics against the provided schema
        anomalies = tfdv.validate_statistics(stats, schema)

        if anomalies.anomaly_info:
            print("Data anomalies detected:")
            for feature_name, anomaly in anomalies.anomaly_info.items():
                print(f"Feature '{feature_name}' - {anomaly.description}")
        else:
            print("Data conforms to the schema.")
    except Exception as e:
        print("Error during data validation:", e)

# Run schema generation and validation
try:
    # Ensure 'df' is a properly formatted DataFrame
    if isinstance(df, pd.DataFrame):
        stats, schema = generate_schema_with_tfdv(df)  # 'df' is the DataFrame containing your data
        validate_data_with_schema(df, schema)
    else:
        print("Input data is not a valid DataFrame.")
except Exception as e:
    print("Unexpected error during schema generation or validation:", e)

import tensorflow_data_validation as tfdv

# Function to generate statistics and infer schema with TFDV
def generate_schema_with_tfdv(data):
    try:
        # Step 1: Generate statistics from the DataFrame
        stats = tfdv.generate_statistics_from_dataframe(data)
        print("Data statistics generated successfully.")

        # Step 2: Infer a schema from the statistics
        schema = tfdv.infer_schema(stats)
        print("Schema inferred successfully.")

        # Adjust display schema with limited text width to prevent rendering errors
        try:
            options = tfdv.display.SchemaRendererOptions(max_string_width=100)  # Set max string width
            tfdv.display_schema(schema, schema_renderer_options=options)
        except Exception as render_error:
            print("Error rendering schema display:", render_error)
            print("Consider inspecting schema text instead.")

        return stats, schema
    except Exception as e:
        print("Error in generating statistics or schema:", e)
        return None, None

# Function to validate data using inferred schema
def validate_data_with_schema(data, schema):
    if schema is None:
        print("No valid schema available for validation.")
        return

    try:
        # Generate statistics from the current data
        stats = tfdv.generate_statistics_from_dataframe(data)

        # Validate statistics against the provided schema
        anomalies = tfdv.validate_statistics(stats, schema)

        if anomalies.anomaly_info:
            print("Data anomalies detected:")
            for feature_name, anomaly in anomalies.anomaly_info.items():
                print(f"Feature '{feature_name}' - {anomaly.description}")
        else:
            print("Data conforms to the schema.")
    except Exception as e:
        print("Error during data validation:", e)

# Run schema generation and validation
try:
    # Ensure 'df' is a properly formatted DataFrame
    if isinstance(df, pd.DataFrame):
        stats, schema = generate_schema_with_tfdv(df)  # 'df' is the DataFrame containing your data
        validate_data_with_schema(df, schema)
    else:
        print("Input data is not a valid DataFrame.")
except Exception as e:
    print("Unexpected error during schema generation or validation:", e)

# Save the inferred schema to a file for easy inspection
tfdv.write_schema_text(schema, 'schema.pbtxt')
print("Schema saved to 'schema.pbtxt'. Open this file to inspect the schema structure.")

from google.colab import files

import tensorflow_data_validation as tfdv

# Step 1: Generate statistics from the DataFrame (ensure `df` is your actual DataFrame)
try:
    stats = tfdv.generate_statistics_from_dataframe(df)
    print("Statistics generated successfully.")
except Exception as e:
    print("Error generating statistics:", e)

# Step 2: Infer schema from the statistics
try:
    schema = tfdv.infer_schema(stats)
    print("Schema inferred successfully.")
except Exception as e:
    print("Error inferring schema:", e)

# Step 3: Save schema if successfully generated
if schema:
    try:
        tfdv.write_schema_text(schema, 'schema.pbtxt')
        print("Schema saved to 'schema.pbtxt'. You can download it now.")
    except Exception as e:
        print("Error saving schema:", e)
else:
    print("Schema was not created, so it cannot be saved.")

# Save the schema file
tfdv.write_schema_text(schema, 'schema.pbtxt')
print("Schema saved to 'schema.pbtxt'.")

# Download the file to your local machine
files.download('schema.pbtxt')

# Display simplified schema information directly
for feature in schema.feature:
    print(f"Feature name: {feature.name}")
    print(f"Type: {feature.type}")
    if feature.HasField("int_domain"):
        print("Domain: Integer")
    elif feature.HasField("float_domain"):
        print("Domain: Float")
    elif feature.HasField("string_domain"):
        print("Domain: String")
    print("---")

# Assuming `df` is your data in a pandas DataFrame
stats = tfdv.generate_statistics_from_dataframe(df)
# Visualize statistics for a single dataset
tfdv.visualize_statistics(lhs_statistics=stats, rhs_statistics=None, lhs_name="Data")

import tensorflow_data_validation as tfdv
import pandas as pd

# Function to generate statistics and infer schema with TFDV
def generate_schema_with_tfdv(data):
    try:
        # Step 1: Generate statistics from the DataFrame
        stats = tfdv.generate_statistics_from_dataframe(data)
        print("Data statistics generated successfully.")

        # Step 2: Infer a schema from the statistics
        schema = tfdv.infer_schema(stats)
        print("Schema inferred successfully.")

        # Display schema (for debugging and understanding structure)
        # Note: Uncomment if display functionality works in your environment
        # tfdv.display_schema(schema)

        return stats, schema
    except Exception as e:
        print("Error in generating statistics or schema:", e)
        return None, None

# Function to validate data using inferred schema
def validate_data_with_schema(data, schema):
    if schema is None:
        print("No valid schema available for validation.")
        return

    try:
        # Generate statistics from the current data
        stats = tfdv.generate_statistics_from_dataframe(data)

        # Validate statistics against the provided schema
        anomalies = tfdv.validate_statistics(stats, schema)

        if anomalies.anomaly_info:
            print("Data anomalies detected:")
            for feature_name, anomaly in anomalies.anomaly_info.items():
                print(f"Feature '{feature_name}' - {anomaly.description}")
        else:
            print("Data conforms to the schema.")
    except Exception as e:
        print("Error during data validation:", e)

# Run schema generation and validation
try:
    # Ensure 'df' is a properly formatted DataFrame
    if isinstance(df, pd.DataFrame):
        stats, schema = generate_schema_with_tfdv(df)  # 'df' is the DataFrame containing your data
        validate_data_with_schema(df, schema)

        # Visualization of the statistics
        print("Visualizing data statistics...")
        tfdv.visualize_statistics(lhs_statistics=stats, lhs_name="Dataset Statistics")
    else:
        print("Input data is not a valid DataFrame.")
except Exception as e:
    print("Unexpected error during schema generation or validation:", e)

# Save schema to a text file for inspection
schema_path = 'schema.pbtxt'
tfdv.write_schema_text(schema, schema_path)
print(f"Schema written to {schema_path}")

"""###  Check and report anomalies


"""

import tensorflow_data_validation as tfdv

# Step 1: Generate statistics from the dataset
stats = tfdv.generate_statistics_from_dataframe(df)

# Step 2: Infer schema from statistics
schema = tfdv.infer_schema(stats)
print("Schema inferred successfully.")

# Step 3: Validate the dataset against the inferred schema to detect anomalies
anomalies = tfdv.validate_statistics(stats, schema)

# Step 4: Check and report anomalies
if anomalies.anomaly_info:
    print("Anomalies detected in the data:")
    for feature_name, anomaly_info in anomalies.anomaly_info.items():
        print(f"Feature '{feature_name}' - {anomaly_info.description}")
else:
    print("No anomalies found; data conforms to the schema.")

import tensorflow_data_validation as tfdv

# Function to load the saved schema
def load_schema(schema_path='/content/drive/My Drive/Colab Notebooks/schema.pbtxt'):
    try:
        schema = tfdv.load_schema_text(schema_path)
        print("Schema loaded successfully.")
        return schema
    except Exception as e:
        print("Error loading schema:", e)

# Function to detect drift and skew between baseline and new data
def check_drift_and_skew(baseline_data, new_data, schema):
    try:
        print("Generating statistics for baseline data...")
        baseline_stats = tfdv.generate_statistics_from_dataframe(baseline_data)
        print("Baseline statistics generated.")

        print("Generating statistics for new data...")
        new_stats = tfdv.generate_statistics_from_dataframe(new_data)
        print("New data statistics generated.")

        print("Validating new data statistics against schema and baseline statistics...")
        drift_anomalies = tfdv.validate_statistics(new_stats, schema, previous_statistics=baseline_stats)

        if drift_anomalies and drift_anomalies.anomaly_info:
            print("Drift or skew detected in the data:")
            for feature_name, anomaly in drift_anomalies.anomaly_info.items():
                print(f"Feature '{feature_name}': {anomaly.description}")
        else:
            print("No drift or skew detected in the data.")
    except Exception as e:
        print("Error during drift and skew detection:", e)

# Usage example
# Ensure that your baseline_data and new_data DataFrames are loaded correctly
schema = load_schema()  # Load the schema from the saved file

# Replace `baseline_df` and `new_df` with your actual DataFrames for baseline and new data
if schema is not None:
    check_drift_and_skew(baseline_df, new_df, schema)
else:
    print("Schema could not be loaded.")

# Add new features or make adjustments as needed
tfdv.write_schema_text(schema, '/content/drive/My Drive/Colab Notebooks/updated_schema.pbtxt')
print("Updated schema saved.")

import tensorflow_data_validation as tfdv
from tensorflow_metadata.proto.v0 import schema_pb2

# Load the current schema (assuming you have an existing schema file)
schema = tfdv.load_schema_text('/content/drive/My Drive/Colab Notebooks/updated_schema.pbtxt')

# Freeze the schema by setting the `in_environment` property for each feature
for feature in schema.feature:
    feature.in_environment.append('PRODUCTION')  # Freeze schema for a production environment
    print(f"Freezing feature '{feature.name}' in 'PRODUCTION' environment.")

# Save the frozen schema back to a file
tfdv.write_schema_text(schema, '/content/drive/My Drive/Colab Notebooks/frozen_schema.pbtxt')
print("Schema frozen and saved to 'frozen_schema.pbtxt'")



"""## **Stock Price **"""

from google.cloud import storage
import json
import pandas as pd

# Initialize the GCS client
client = storage.Client()
bucket_name = 'stock_prices-bucket'        # Update with your actual bucket name
bucket = client.get_bucket(bucket_name)

# List blobs (files) in the bucket
blobs = bucket.list_blobs()
data_list = []

# Iterate over each blob in the bucket
for blob in blobs:
    # Only process JSON files
    if blob.name.endswith(".json"):
        print(f"Processing file: {blob.name}")
        json_data = blob.download_as_text()
        try:
            # Parse JSON and append data to the list
            data = json.loads(json_data)
            if isinstance(data, list):
                data_list.extend(data)  # Add all records in the list
            elif isinstance(data, dict):
                data_list.append(data)  # Add single record
            else:
                print(f"Unexpected JSON format in {blob.name}, skipping.")
        except json.JSONDecodeError:
            print(f"Error decoding JSON in {blob.name}, skipping.")

# Convert list of dictionaries to a DataFrame if there is data
if data_list:
    df = pd.DataFrame(data_list)
    print("Data loaded into DataFrame successfully.")
    print(df.head())  # Display the first few rows of the DataFrame
else:
    print("No valid JSON data found in the bucket.")

from google.cloud import storage
import json
import pandas as pd

# Initialize the GCS client
client = storage.Client()
bucket_name = 'stock_prices-bucket'  # Update with your actual bucket name
bucket = client.get_bucket(bucket_name)

# List blobs (files) in the bucket
blobs = bucket.list_blobs()
data_list = []
problematic_files = []  # To store the names of files with unexpected formats

# Iterate over each blob in the bucket
for blob in blobs:
    if blob.name.endswith(".json"):  # Only process JSON files
        print(f"Processing file: {blob.name}")
        json_data = blob.download_as_text()
        try:
            # Parse JSON and append data to the list
            data = json.loads(json_data)
            if isinstance(data, list):
                data_list.extend(data)  # Add all records in the list
            elif isinstance(data, dict):
                data_list.append(data)  # Add single record
            else:
                print(f"Unexpected JSON format in {blob.name}, skipping.")
                problematic_files.append(blob.name)
        except json.JSONDecodeError:
            print(f"Error decoding JSON in {blob.name}, skipping.")
            problematic_files.append(blob.name)

# Convert list of dictionaries to a DataFrame if there is data
if data_list:
    df = pd.DataFrame(data_list)
    print("Data loaded into DataFrame successfully.")
    print(df.head())  # Display the first few rows of the DataFrame
else:
    print("No valid JSON data found in the bucket.")

# Print problematic files if any
if problematic_files:
    print("\nFiles with unexpected JSON formats:")
    for file in problematic_files:
        print(file)

from google.cloud import storage
import json
import pandas as pd

# Initialize the GCS client
client = storage.Client()
bucket_name = 'stock_prices-bucket'  # Ensure this is the correct bucket name
bucket = client.get_bucket(bucket_name)

# List blobs (files) in the bucket
blobs = bucket.list_blobs()
data_list = []
problematic_files = []  # Store details of problematic files for later inspection

# Function to determine if JSON is in a valid record format for DataFrame
def is_valid_record(data):
    if isinstance(data, dict):  # Single record
        return True
    elif isinstance(data, list) and all(isinstance(item, dict) for item in data):  # List of records
        return True
    return False

# Process each blob (file) in the bucket
for blob in blobs:
    if blob.name.endswith(".json"):  # Process JSON files only
        print(f"Processing file: {blob.name}")
        json_data = blob.download_as_text()
        try:
            # Parse JSON and check if it's a valid format
            data = json.loads(json_data)
            if is_valid_record(data):
                # Append data to list based on structure
                if isinstance(data, dict):
                    data_list.append(data)
                elif isinstance(data, list):
                    data_list.extend(data)
            else:
                print(f"Unexpected JSON format in {blob.name}, logging for inspection.")
                problematic_files.append((blob.name, json_data))  # Capture raw JSON data for inspection
        except json.JSONDecodeError:
            print(f"Error decoding JSON in {blob.name}, logging for inspection.")
            problematic_files.append((blob.name, "JSONDecodeError"))

# Convert list of dictionaries to a DataFrame if data was collected
if data_list:
    df = pd.DataFrame(data_list)
    print("Data loaded into DataFrame successfully.")
    print(df.head())  # Display the first few rows of the DataFrame
else:
    print("No valid JSON data found in the bucket.")

# Write problematic files to a log file for further inspection
if problematic_files:
    with open("problematic_files_log.json", "w") as log_file:
        json.dump(problematic_files, log_file, indent=2)
    print("Log of problematic files saved as 'problematic_files_log.json' for further inspection.")
else:
    print("No problematic JSON files detected.")

# Display the first few rows
print(df.head())

# Get information about the DataFrame
print(df.info())

# Check for missing values
print("Missing values per column:\n", df.isnull().sum())

import tensorflow_data_validation as tfdv

# Generate statistics from the DataFrame
stats = tfdv.generate_statistics_from_dataframe(df)

# Infer schema from the statistics
schema = tfdv.infer_schema(stats)

# Display the inferred schema
tfdv.display_schema(schema)

# Generate statistics from the DataFrame
stats = tfdv.generate_statistics_from_dataframe(df)
print("Data statistics generated successfully.")

# Display the generated statistics for inspection (optional, in Jupyter/Colab only)
tfdv.visualize_statistics(stats)

# Infer schema based on the generated statistics
schema = tfdv.infer_schema(stats)
print("Schema inferred successfully.")

# Display the inferred schema for inspection (optional)
tfdv.display_schema(schema)

# Validate statistics against the inferred schema
anomalies = tfdv.validate_statistics(stats, schema)

# Check for anomalies
if anomalies.anomaly_info:
    print("Anomalies detected:")
    for feature_name, anomaly_info in anomalies.anomaly_info.items():
        print(f"Feature '{feature_name}': {anomaly_info.description}")
else:
    print("No anomalies detected; data conforms to the schema.")

# Save the schema to a file
schema_path='/content/drive/My Drive/Colab Notebooks/stock_price_schema.pbtxt'

tfdv.write_schema_text(schema, schema_path)
print(f"Schema saved to '{schema_path}'.")

"""
# Load the schema
loaded_schema = tfdv.load_schema_text(schema_path)

# Generate new statistics for the next dataset and validate against the loaded schema
new_stats = tfdv.generate_statistics_from_dataframe(new_df)  # replace `new_df` with the new data
new_anomalies = tfdv.validate_statistics(new_stats, loaded_schema)

if new_anomalies.anomaly_info:
    print("Anomalies detected in new data:")
    for feature_name, anomaly_info in new_anomalies.anomaly_info.items():
        print(f"Feature '{feature_name}': {anomaly_info.description}")
else:
    print("No anomalies detected in the new data.")
"""

# Step 1: Generate Statistics and Schema
stats = tfdv.generate_statistics_from_dataframe(df)
schema = tfdv.infer_schema(stats)

# Validate against inferred schema
anomalies = tfdv.validate_statistics(stats, schema)
if anomalies.anomaly_info:
    print("Schema conformance anomalies detected:")
    for feature_name, anomaly_info in anomalies.anomaly_info.items():
        print(f"Feature '{feature_name}': {anomaly_info.description}")
else:
    print("Data conforms to the schema.")

# Save the schema for future use
tfdv.write_schema_text(schema, 'schema.pbtxt')
print("Schema saved to 'schema.pbtxt'.")

# Step 2: Check for Missing Values
missing_values = df.isnull().sum()
print("Missing values per column:\n", missing_values)

# Step 3: Range Validation
price_range = (0, 2000)  # Adjust as necessary
volume_min = 0

for col in ['Open', 'High', 'Low', 'Close']:
    if ((df[col] < price_range[0]) | (df[col] > price_range[1])).any():
        print(f"Anomaly detected: {col} contains values outside the range {price_range}.")
    else:
        print(f"{col} values are within the expected range.")

if (df['Volume'] < volume_min).any():
    print("Anomaly detected: Negative volume found.")
else:
    print("Volume values are valid.")

# Step 4: Anomaly Detection with TFDV
anomalies = tfdv.validate_statistics(stats, schema)
if anomalies.anomaly_info:
    print("Anomalies detected by TFDV:")
    for feature_name, anomaly in anomalies.anomaly_info.items():
        print(f"Feature '{feature_name}' - {anomaly.description}")
else:
    print("No TFDV-detected anomalies; data conforms to the schema.")

# Step 5: Custom Domain-Specific Validations

# Convert Datetime column to Pandas datetime format and check for format issues
df['Datetime'] = pd.to_datetime(df['Datetime'], errors='coerce')
if df['Datetime'].isnull().any():
    print("Anomaly detected: Some datetime values are incorrectly formatted.")
else:
    print("All datetime values are correctly formatted.")

# Check for duplicate datetime entries
if df['Datetime'].duplicated().any():
    print("Anomaly detected: Duplicate datetime entries found.")
else:
    print("All datetime entries are unique.")

# Ensure High >= Open and Close, and Low <= Open and Close
if not ((df['High'] >= df['Open']) & (df['High'] >= df['Close'])).all():
    print("Anomaly detected: 'High' price should be greater than or equal to 'Open' and 'Close'.")
if not ((df['Low'] <= df['Open']) & (df['Low'] <= df['Close'])).all():
    print("Anomaly detected: 'Low' price should be less than or equal to 'Open' and 'Close'.")

# Step 6: Distribution and Skew Analysis (Optional)
# Example baseline data for drift detection
baseline_df = pd.DataFrame(data_list)  # Replace with historical data DataFrame if available
baseline_stats = tfdv.generate_statistics_from_dataframe(baseline_df)  # historical data

drift_anomalies = tfdv.validate_statistics(stats, schema, previous_statistics=baseline_stats)
if drift_anomalies.anomaly_info:
    print("Drift or skew detected:")
    for feature_name, anomaly in drift_anomalies.anomaly_info.items():
        print(f"Feature '{feature_name}' - {anomaly.description}")
else:
    print("No drift or skew detected.")

duplicate_rows = df[df['Datetime'].duplicated(keep=False)]
print("Duplicate datetime entries:")
display(duplicate_rows)



df = df.drop_duplicates(subset=['Datetime'])
df['is_duplicate'] = df['Datetime'].duplicated(keep=False)
import pandas as pd
import numpy as np

# Assuming `df` is your loaded DataFrame

# Step 1: Statistical Summary
print("Statistical Summary of Numeric Columns:")
display(df.describe())

# Step 2: Set `Datetime` as a DateTime type and Sort Data
df['Datetime'] = pd.to_datetime(df['Datetime'])
df = df.sort_values('Datetime').reset_index(drop=True)
print("\nDatetime column converted and data sorted by Datetime.")

# Step 3: Rolling Statistics - Moving Averages for `Close` Price
df['Close_7day_MA'] = df['Close'].rolling(window=7).mean()
df['Close_30day_MA'] = df['Close'].rolling(window=30).mean()
print("\n7-day and 30-day moving averages for 'Close' price added to DataFrame.")

# Step 4: Detect Sudden Price Changes (Anomalies)
df['Price_Change'] = df['Close'].pct_change() * 100  # Percentage change
anomalies = df[np.abs(df['Price_Change']) > 5]  # Threshold of 5% for anomaly detection
print("\nPotential anomalies detected (Price Change > 5%):")
display(anomalies[['Datetime', 'Close', 'Price_Change']])

# Optional: Handle or flag anomalies
df['is_anomaly'] = np.where(np.abs(df['Price_Change']) > 5, True, False)
print("\nAnomaly column added to DataFrame for rows with large price changes.")

# Step 5: Save the processed DataFrame (if needed)
df.to_csv("processed_stock_data.csv", index=False)
display("Processed data saved as 'processed_stock_data.csv'.")

import tensorflow_data_validation as tfdv
import pandas as pd
from google.cloud import storage
import json
from google.protobuf import text_format  # Importing text_format to save schema

# Initialize GCS client
client = storage.Client()
bucket_name = 'stock_prices-bucket'
bucket = client.get_bucket(bucket_name)

# Step 1: Load and Prepare Data
def load_data_from_gcs(bucket, prefix="real_time/AAPL"):
    blobs = bucket.list_blobs(prefix=prefix)
    data_list = []
    for blob in blobs:
        if blob.name.endswith(".json"):
            json_data = blob.download_as_text()
            try:
                data = json.loads(json_data)
                if isinstance(data, dict):
                    data_list.append(data)
                elif isinstance(data, list):
                    data_list.extend(data)
            except json.JSONDecodeError:
                print(f"Error decoding JSON in {blob.name}")
    return pd.DataFrame(data_list)

# Load data into a DataFrame
df = load_data_from_gcs(bucket)

# Step 2: Generate TFDV Statistics
stats = tfdv.generate_statistics_from_dataframe(df)
print("Statistics generated.")

# Step 3: Infer Initial Schema
schema = tfdv.infer_schema(stats)
print("Schema inferred.")

# Save schema for later use
tfdv.write_schema_text(schema, 'schema.pbtxt')
print("Schema saved to 'schema.pbtxt'.")

# Step 4: Validate Current Data Against Schema
anomalies = tfdv.validate_statistics(stats, schema)
if anomalies.anomaly_info:
    print("Anomalies detected:")
    for feature_name, anomaly in anomalies.anomaly_info.items():
        print(f"{feature_name}: {anomaly.description}")
else:
    print("Data conforms to the schema.")

# Step 5: Set Up Schema Environments for Production (Alternative Approach)
# Here we assign each feature to the "PRODUCTION" environment
for feature in schema.feature:
    feature.in_environment.append('PRODUCTION')
print("Environment 'PRODUCTION' set up for all features.")

# Step 6: Detect Data Drift and Skew (between two data sources)
def check_drift_and_skew(baseline_data, new_data, schema):
    baseline_stats = tfdv.generate_statistics_from_dataframe(baseline_data)
    new_stats = tfdv.generate_statistics_from_dataframe(new_data)
    drift_anomalies = tfdv.validate_statistics(new_stats, schema, previous_statistics=baseline_stats)
    if drift_anomalies.anomaly_info:
        print("Drift or skew detected:")
        for feature_name, anomaly in drift_anomalies.anomaly_info.items():
            print(f"{feature_name}: {anomaly.description}")
    else:
        print("No drift or skew detected.")

# Example Usage: Check drift between two datasets
# Assuming `baseline_df` and `new_df` are DataFrames for baseline and new data
# check_drift_and_skew(baseline_df, new_df, schema)

# Step 7: Save the Frozen Schema for Production
with open("frozen_schema.pbtxt", "w") as f:
    f.write(text_format.MessageToString(schema))
print("Frozen schema saved to 'frozen_schema.pbtxt'.")

import tensorflow_data_validation as tfdv
import pandas as pd
from google.cloud import storage
import json

# Initialize GCS client
client = storage.Client()
bucket_name = 'stock_prices-bucket'
bucket = client.get_bucket(bucket_name)

# Step 1: Load and Prepare Data
def load_data_from_gcs(bucket, prefix="real_time/AAPL"):
    blobs = bucket.list_blobs(prefix=prefix)
    data_list = []
    for blob in blobs:
        if blob.name.endswith(".json"):
            json_data = blob.download_as_text()
            try:
                data = json.loads(json_data)
                if isinstance(data, dict):
                    data_list.append(data)
                elif isinstance(data, list):
                    data_list.extend(data)
            except json.JSONDecodeError:
                print(f"Error decoding JSON in {blob.name}")
    return pd.DataFrame(data_list)

# Load data into a DataFrame
df = load_data_from_gcs(bucket)

# Step 2: Generate TFDV Statistics
stats = tfdv.generate_statistics_from_dataframe(df)
print("Statistics generated.")

# Step 3: Infer Initial Schema
schema = tfdv.infer_schema(stats)
print("Schema inferred.")

# Save schema for later use
tfdv.write_schema_text(schema, 'schema.pbtxt')
print("Schema saved to 'schema.pbtxt'.")

# Step 4: Validate Current Data Against Schema
anomalies = tfdv.validate_statistics(stats, schema)
if anomalies.anomaly_info:
    print("Anomalies detected:")
    for feature_name, anomaly in anomalies.anomaly_info.items():
        print(f"{feature_name}: {anomaly.description}")
else:
    print("Data conforms to the schema.")

# Step 5: Set Up Schema Environments for Production (Alternative Approach)
# Here we assign each feature to the "PRODUCTION" environment
for feature in schema.feature:
    feature.in_environment.append('PRODUCTION')
print("Environment 'PRODUCTION' set up for all features.")

# Step 6: Detect Data Drift and Skew (between two data sources)
def check_drift_and_skew(baseline_data, new_data, schema):
    baseline_stats = tfdv.generate_statistics_from_dataframe(baseline_data)
    new_stats = tfdv.generate_statistics_from_dataframe(new_data)
    drift_anomalies = tfdv.validate_statistics(new_stats, schema, previous_statistics=baseline_stats)
    if drift_anomalies.anomaly_info:
        print("Drift or skew detected:")
        for feature_name, anomaly in drift_anomalies.anomaly_info.items():
            print(f"{feature_name}: {anomaly.description}")
    else:
        print("No drift or skew detected.")

# Example Usage: Check drift between two datasets
# Assuming `baseline_df` and `new_df` are DataFrames for baseline and new data
# check_drift_and_skew(baseline_df, new_df, schema)

# Step 7: Save the Frozen Schema for Production
tfdv.write_schema_text(schema, 'frozen_schema.pbtxt')
print("Frozen schema saved to 'frozen_schema.pbtxt'.")
